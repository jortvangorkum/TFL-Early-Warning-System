{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "\n",
    "merged_data = pandas.read_pickle('../data/data-preperation-output.pickle')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# analyze the max weight of a course\n",
    "assessment_lists_per_student_per_course = merged_data.groupby(['code_module', 'code_presentation']).agg({\n",
    "    \"assessments\": lambda x: list(x),\n",
    "}).reset_index()\n",
    "\n",
    "def analyze_max_weight_per_course_per_presentation(row):\n",
    "    asessments_of_students = row['assessments']\n",
    "    max_weight = 0\n",
    "    for asessments_of_student in asessments_of_students:\n",
    "        weights = [weight for (id_assessment, date_submitted, score, assessment_type, date, weight) in asessments_of_student]\n",
    "        student_max_weight = sum(weights)\n",
    "        if student_max_weight > max_weight:\n",
    "            max_weight = student_max_weight\n",
    "        \n",
    "\n",
    "    row['max_weight'] = max_weight\n",
    "    return row\n",
    "\n",
    "max_weights_per_course_per_presentation = assessment_lists_per_student_per_course.apply(analyze_max_weight_per_course_per_presentation, axis = 1)\n",
    "max_weights_per_course_per_presentation = max_weights_per_course_per_presentation.drop('assessments', axis=1).set_index(['code_module', 'code_presentation'])\n",
    "max_weights_per_course_per_presentation.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_average_grade(assessments):\n",
    "    total_weight = 0\n",
    "    total_score = 0\n",
    "\n",
    "    for assessment in assessments:\n",
    "        id_assessment, date_submitted, score, assessment_type, date, weight = assessment\n",
    "        if (not(type(score) == float)):\n",
    "            print(score)\n",
    "        total_weight +=  weight\n",
    "        total_score += score * weight\n",
    "\n",
    "\n",
    "    if total_weight > 0:\n",
    "        average_score = total_score / total_weight\n",
    "        return average_score\n",
    "    else:\n",
    "        return math.nan\n",
    "\n",
    "merged_data['average_score'] = merged_data['assessments'].transform(analyze_average_grade)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jort\n",
    "# sum of all clicks\n",
    "\n",
    "def sum_clicks_vles(vles):\n",
    "    sum_clicks = 0\n",
    "    for [id_site, date, activity_type, sum_click] in vles:\n",
    "        sum_clicks += sum_click\n",
    "    return sum_clicks\n",
    "\n",
    "merged_data[\"sum_clicks\"] = merged_data[\"vles\"].transform(sum_clicks_vles)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop([\"assessments\", \"vles\"], axis=1)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_pickle(\"../data/data-extension-output.pickle\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e475ca911d037dd20b3a468ef59286c04ab37dedf18160dcce0c9f36ca56fa54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python395jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}